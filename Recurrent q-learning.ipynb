{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271eb981-34c5-4be9-a265-8e0652527cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot\n",
    "import  matplotlib.pyplot as plt\n",
    "from model import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde87488-9a43-406d-8766-4b324ec84c46",
   "metadata": {},
   "source": [
    "### Init enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f305df64-51bb-48a3-a169-437ef2858d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61269f8b-cf59-4426-88e4-608bd01108d6",
   "metadata": {},
   "source": [
    "### Init agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163364e8-2d9c-4efd-bc9b-3d9bee3b06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cfb24-f348-4d6d-b729-c8fa7c1fabe9",
   "metadata": {},
   "source": [
    "### Learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701615a7-25e1-40a2-88be-6d21ebb8d9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 avg reward (last 100): 12.0 epsilon 0.9995\n",
      "episode: 100 avg reward (last 100): 23.62 epsilon 0.9507419214772129\n",
      "episode: 200 avg reward (last 100): 22.8 epsilon 0.9043623824454068\n",
      "episode: 300 avg reward (last 100): 22.42 epsilon 0.8602453518737936\n",
      "episode: 400 avg reward (last 100): 23.03 epsilon 0.8182804590118382\n",
      "episode: 500 avg reward (last 100): 24.26 epsilon 0.7783627172668048\n",
      "episode: 600 avg reward (last 100): 23.75 epsilon 0.7403922615512425\n",
      "episode: 700 avg reward (last 100): 21.63 epsilon 0.7042740984433092\n",
      "episode: 800 avg reward (last 100): 30.8 epsilon 0.6699178685348912\n",
      "episode: 900 avg reward (last 100): 35.16 epsilon 0.6372376203729679\n",
      "episode: 1000 avg reward (last 100): 20.04 epsilon 0.606151595428677\n",
      "episode: 1100 avg reward (last 100): 18.23 epsilon 0.5765820235561167\n",
      "episode: 1200 avg reward (last 100): 20.11 epsilon 0.5484549284291772\n",
      "episode: 1300 avg reward (last 100): 14.45 epsilon 0.5216999424696379\n",
      "episode: 1400 avg reward (last 100): 18.55 epsilon 0.4962501308035369\n",
      "episode: 1500 avg reward (last 100): 20.37 epsilon 0.47204182380537574\n",
      "episode: 1600 avg reward (last 100): 13.75 epsilon 0.4490144578112366\n",
      "episode: 1700 avg reward (last 100): 25.54 epsilon 0.427110423602305\n",
      "episode: 1800 avg reward (last 100): 12.9 epsilon 0.40627492227974177\n",
      "episode: 1900 avg reward (last 100): 13.2 epsilon 0.3864558281703322\n",
      "episode: 2000 avg reward (last 100): 15.85 epsilon 0.36760355841993914\n",
      "episode: 2100 avg reward (last 100): 33.89 epsilon 0.349670948948508\n",
      "episode: 2200 avg reward (last 100): 11.97 epsilon 0.3326131364562932\n",
      "episode: 2300 avg reward (last 100): 11.8 epsilon 0.31638744618611153\n",
      "episode: 2400 avg reward (last 100): 12.7 epsilon 0.30095328516083264\n",
      "episode: 2500 avg reward (last 100): 16.49 epsilon 0.2862720406290043\n",
      "episode: 2600 avg reward (last 100): 31.61 epsilon 0.2723069834645546\n",
      "episode: 2700 avg reward (last 100): 41.41 epsilon 0.25902317627889354\n",
      "episode: 2800 avg reward (last 100): 38.36 epsilon 0.24638738601553387\n",
      "episode: 2900 avg reward (last 100): 31.07 epsilon 0.23436800080856074\n",
      "episode: 3000 avg reward (last 100): 28.57 epsilon 0.22293495089695248\n",
      "episode: 3100 avg reward (last 100): 29.87 epsilon 0.21205963339689513\n",
      "episode: 3200 avg reward (last 100): 20.96 epsilon 0.20171484074388943\n",
      "episode: 3300 avg reward (last 100): 23.76 epsilon 0.19187469262562828\n",
      "episode: 3400 avg reward (last 100): 23.8 epsilon 0.1825145712353572\n",
      "episode: 3500 avg reward (last 100): 22.84 epsilon 0.1736110596837351\n",
      "episode: 3600 avg reward (last 100): 23.16 epsilon 0.16514188341511685\n",
      "episode: 3700 avg reward (last 100): 23.39 epsilon 0.15708585448169488\n",
      "episode: 3800 avg reward (last 100): 24.26 epsilon 0.14942281853608433\n",
      "episode: 3900 avg reward (last 100): 27.34 epsilon 0.14213360440974254\n",
      "episode: 4000 avg reward (last 100): 27.37 epsilon 0.1351999761510762\n",
      "episode: 4100 avg reward (last 100): 26.94 epsilon 0.12860458740324937\n",
      "episode: 4200 avg reward (last 100): 28.78 epsilon 0.12233093800755339\n",
      "episode: 4300 avg reward (last 100): 29.07 epsilon 0.11636333272377304\n",
      "episode: 4400 avg reward (last 100): 32.21 epsilon 0.11068684196427431\n",
      "episode: 4500 avg reward (last 100): 31.25 epsilon 0.10528726444358047\n",
      "episode: 4600 avg reward (last 100): 32.99 epsilon 0.10015109164999415\n",
      "episode: 4700 avg reward (last 100): 24.68 epsilon 0.09526547405038113\n",
      "episode: 4800 avg reward (last 100): 33.59 epsilon 0.09061818894356875\n",
      "episode: 4900 avg reward (last 100): 34.43 epsilon 0.08619760988193471\n",
      "episode: 5000 avg reward (last 100): 35.67 epsilon 0.08199267758468616\n",
      "episode: 5100 avg reward (last 100): 40.87 epsilon 0.0779928722700611\n",
      "episode: 5200 avg reward (last 100): 34.56 epsilon 0.07418818733723333\n",
      "episode: 5300 avg reward (last 100): 21.8 epsilon 0.07056910433207873\n",
      "episode: 5400 avg reward (last 100): 43.81 epsilon 0.06712656913417349\n",
      "episode: 5500 avg reward (last 100): 39.84 epsilon 0.06385196930544977\n",
      "episode: 5600 avg reward (last 100): 40.55 epsilon 0.06073711254383927\n",
      "episode: 5700 avg reward (last 100): 37.15 epsilon 0.0577742061880015\n",
      "episode: 5800 avg reward (last 100): 31.92 epsilon 0.054955837721862175\n",
      "episode: 5900 avg reward (last 100): 31.11 epsilon 0.052274956230188276\n",
      "episode: 6000 avg reward (last 100): 27.45 epsilon 0.04972485475880578\n",
      "episode: 6100 avg reward (last 100): 18.09 epsilon 0.04729915353532999\n",
      "episode: 6200 avg reward (last 100): 18.39 epsilon 0.04499178400842957\n",
      "episode: 6300 avg reward (last 100): 15.02 epsilon 0.04279697366569491\n",
      "episode: 6400 avg reward (last 100): 17.49 epsilon 0.04070923159212851\n",
      "episode: 6500 avg reward (last 100): 12.01 epsilon 0.03872333473312768\n",
      "episode: 6600 avg reward (last 100): 12.71 epsilon 0.0368343148275929\n",
      "episode: 6700 avg reward (last 100): 10.55 epsilon 0.03503744597847152\n",
      "episode: 6800 avg reward (last 100): 11.05 epsilon 0.03332823282964087\n",
      "episode: 6900 avg reward (last 100): 12.15 epsilon 0.031702399319552474\n",
      "episode: 7000 avg reward (last 100): 11.65 epsilon 0.030155877983500955\n",
      "episode: 7100 avg reward (last 100): 12.66 epsilon 0.028684799777754958\n",
      "episode: 7200 avg reward (last 100): 11.36 epsilon 0.027285484400091944\n",
      "episode: 7300 avg reward (last 100): 15.51 epsilon 0.025954431082521167\n",
      "episode: 7400 avg reward (last 100): 14.98 epsilon 0.02468830983316065\n",
      "episode: 7500 avg reward (last 100): 14.57 epsilon 0.023483953105356593\n",
      "episode: 7600 avg reward (last 100): 14.83 epsilon 0.022338347873204104\n",
      "episode: 7700 avg reward (last 100): 20.58 epsilon 0.021248628093643293\n",
      "episode: 7800 avg reward (last 100): 16.78 epsilon 0.020212067536273238\n",
      "episode: 7900 avg reward (last 100): 29.8 epsilon 0.019226072962945095\n",
      "episode: 8000 avg reward (last 100): 26.13 epsilon 0.018288177640071544\n",
      "episode: 8100 avg reward (last 100): 18.42 epsilon 0.01739603516742193\n",
      "episode: 8200 avg reward (last 100): 21.62 epsilon 0.016547413607963874\n",
      "episode: 8300 avg reward (last 100): 19.67 epsilon 0.01574018990406579\n",
      "episode: 8400 avg reward (last 100): 10.58 epsilon 0.014972344566090779\n",
      "episode: 8500 avg reward (last 100): 22.54 epsilon 0.014241956620094096\n",
      "episode: 8600 avg reward (last 100): 37.82 epsilon 0.013547198801984371\n",
      "episode: 8700 avg reward (last 100): 25.18 epsilon 0.012886332986125485\n",
      "episode: 8800 avg reward (last 100): 25.75 epsilon 0.012257705836942604\n",
      "episode: 8900 avg reward (last 100): 36.57 epsilon 0.011659744672653587\n",
      "episode: 9000 avg reward (last 100): 34.02 epsilon 0.011090953530777752\n",
      "episode: 9100 avg reward (last 100): 19.24 epsilon 0.010549909425578914\n",
      "episode: 9200 avg reward (last 100): 10.75 epsilon 0.010035258788079503\n",
      "episode: 9300 avg reward (last 100): 43.58 epsilon 0.01\n",
      "episode: 9400 avg reward (last 100): 46.51 epsilon 0.01\n",
      "episode: 9500 avg reward (last 100): 52.33 epsilon 0.01\n",
      "episode: 9600 avg reward (last 100): 35.06 epsilon 0.01\n",
      "episode: 9700 avg reward (last 100): 38.89 epsilon 0.01\n",
      "episode: 9800 avg reward (last 100): 50.1 epsilon 0.01\n",
      "episode: 9900 avg reward (last 100): 41.43 epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "l = []\n",
    "losses = []\n",
    "for i in range(10000):\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    observation = np.around(observation, decimals = 4)\n",
    "\n",
    "\n",
    "    states_list=[]\n",
    "    states_next_list=[]\n",
    "    rewards_list=[]\n",
    "    actions_list=[]\n",
    "    dones_list=[]\n",
    "\n",
    "    agent.model.eval()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        \n",
    "        states_list.append(observation)\n",
    "\n",
    "        action = agent.choose_action(state=observation)\n",
    "        prev_observation = observation\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation =np.around(observation , decimals = 4)\n",
    "\n",
    "        rewards += reward\n",
    "        \n",
    "        states_next_list.append(observation)\n",
    "        rewards_list.append(reward)\n",
    "        actions_list.append(action)\n",
    "        dones_list.append(done)\n",
    "        steps += 1\n",
    "\n",
    "        \n",
    "    loss = agent.train_(states_list = states_list,\n",
    "                        actions_list = actions_list,\n",
    "                        rewards_list = rewards_list,\n",
    "                        states_next_list = states_next_list,\n",
    "                        dones_list = dones_list)\n",
    "\n",
    "    agent.change_eps()\n",
    "    l.append(rewards)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"episode:\", i, \"avg reward (last 100):\",np.average(l[-100:]), 'epsilon',agent.eps)\n",
    "        \n",
    "    writer.add_scalar('Loss/train', loss, i)\n",
    "    writer.add_scalar('Reward/train', rewards, i)\n",
    "    writer.add_scalar('Episod lenght/train', steps, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
